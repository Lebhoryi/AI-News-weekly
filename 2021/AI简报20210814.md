[TOC]

> 导读： 本期为 AI 简报 20210814 期，请慢用~
>
> AI 简报 Github 地址：https://github.com/Lebhoryi/AI-News-weekly
>
> 本文一共 2000 字，通篇阅读结束需要 7~10 分钟

# AI 前沿

## 1.【轻量化】[Mobile-Former来了！微软提出：MobileNet+Transformer轻量化并行网络](https://mp.weixin.qq.com/s/m4rqDk-JG2Kb9aJDVG8OHQ) |  集智书童

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210814180017.png)

> Github: https://arxiv.org/abs/2108.05895

最近，Vision Transformer(ViT)展示了全局处理的优势，与cnn相比实现了显著的性能提升。然而，当将计算预算限制在1G FLOPs内时，增益维特减少。如果进一步挑战计算成本，基于depthwise和pointwise卷积的MobileNet和它的扩展仍然占据着一席之地(例如，少于300M的FLOPs图像分类),这又自然而然地提出了一个问题:

如何设计有效的网络来有效地编码局部处理和全局交互?

一个简单的想法是将卷积和Vision Transformer结合起来。最近的研究表明，将卷积和Vision Transformer串联在一起，无论是在开始时使用卷积，还是将卷积插入到每个Transformer块中都是有益的。

在本文中，作者将设计范式从串联向并联转变，提出了一种新的MobileNet和Transformer并行化，并在两者之间建立双向桥接(见图)。将其命名为**Mobile-Former**，其中Mobile指MobileNet, Former指transformer。Mobile以图像为输入堆叠mobile block(或inverted bottleneck)。它利用高效的depthwise和pointwise卷积来提取像素级的局部特征。前者以一些可学习的token作为输入，叠加multi-head attention和前馈网络(FFN)。这些token用于对图像的全局特征进行编码。

Mobile-Former是MobileNet和Transformer的并行设计，中间有一个双向桥接。这种结构利用了MobileNet在局部处理和Transformer在全局交互方面的优势。并且该桥接可以实现局部和全局特征的双向融合。与最近在视觉Transformer上的工作不同，Mobile-Former中的Transformer包含非常少的随机初始化的token（例如少于6个token），从而导致计算成本低。

## 2. [ICCV'21 Oral｜拒绝调参，显著提点！检测分割任务的新损失函数RS Loss开源](https://mp.weixin.qq.com/s/OPw4uGc_VFtHt98SgC1yJg)

![Image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoiadhClDmqt2TYUb0mcRG9S7ejG96407vPysA6SOdB0CUR2MY1AicOXgOG4cBZQ3Xxo1JJzpic98aJA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



> 论文地址：https://arxiv.org/abs/2107.11669
>
> 代码地址：https://github.com/kemaloksuz/RankSortLoss

目标检测和实例分割往往是一个multi-task的任务，其中包含了诸如classification，box regression和mask prediction等多个子任务，因此对于这类任务的损失函数往往是多个子任务的损失函数来加权求和，如下所示：

![Image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoiadhClDmqt2TYUb0mcRG9S71HvPPFzd25xv36pNfyD1ZwTRTOJl6UUEnkgeoicRV9Wo75lYbeGCfA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中，是在第k个stage中第t个任务的损失函数（对于Faster R-CNN这类二阶段的目标检测器，就等于2），是一个用来决定每个stage中每个任务权重的超参数。

由于子任务和stage的多样性以及每个任务重要性的不平衡，在这类任务中，超参数的数量往往就会比较多。虽然加入这些超参数来平衡不同任务的重要性能够让模型获得更好的性能，但由于调参是非常耗时耗资源的，并且次优的超参数会导致模型次优的性能。

因此，在本文中，作者提出了一种用于目标检测和实例分割任务的Rank & Sort Loss（RSLoss），能够简化原来模型训练的复杂性，并能使得模型达到更好的performance。实验中，作者在7个模型中验证了RSLoss的有效性。



## 3.【亚军方案】【开源】[平安产险提出TableMASTER：表格识别大师](https://mp.weixin.qq.com/s/RIQkj4xM5DEjxhMRlPjtXw) | CVer

![image-20210814172523640](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210814172524.png)

> 论文：https://arxiv.org/abs/2105.01848
>
> GitHub代码链接：https://github.com/JiaquanYe/TableMASTER-mmocr
>
> MASTER代码链接：https://github.com/JiaquanYe/MASTER-mmocr

表格作为传递信息的文本格式之一，在文献（尤其是科学文献）中广泛存在。在 ICDAR2021 科学文献解析表格Table2HTML 比赛中，参赛选手需要设计一个算法模型或者算法系统，把包含表格数据的图像，转为HTML代码。平安财产保险视觉计算团队作为本次比赛参赛选手，提出了**TableMASTER**算法模型，采用多任务学习的模式，同时进行表格结构序列预测以及单元格位置回归，最后通过后处理匹配算法，融合表格结构序列和单元格文本内容，得到HTML代码。最终，团队以**96.32** TEDS score的成绩，取得了该赛道的亚军。比赛的详细介绍可点击链接：

https://aieval.draco.res.ibm.com/challenge/40/overview

目前，作者团队基于开源工具箱mmocr，复现了该解决方案，代码已开源！

# 有趣的开源项目

## 4. [比用Pytorch框架快200倍！0.76秒后，笔记本上的CNN就搞定了MNIST | 开源](https://mp.weixin.qq.com/s/hWv4Cv4O3Bs7O1EEVCL4eQ) | 量子位

![image-20210814172900341](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210814172901.png)

> Github: https://github.com/tuomaso/train_mnist_fast

在MNIST上进行训练，可以说是计算机视觉里的“Hello World”任务了。

而如果使用PyTorch的标准代码训练CNN，一般需要3分钟左右。

但现在，在一台**笔记本电脑**上就能将时间缩短200多倍。

速度直达**0.76秒**！

那么，到底是如何仅在一次epoch的训练中就达到99%的准确率的呢？

先下载数据集进行训练，每次运行训练14个epoch。

这时两次运行的平均准确率在测试集上为99.185%，平均运行时间为2min 52s ± 38.1ms。

接下来，就是一步一步来减少训练时间

## 5. [推理成本降低48倍！1张GPU就能让静态图像动起来](https://mp.weixin.qq.com/s/S0CgyNLo4Lvej9CKTj1b_g) | 量子位

![image-20210814173631670](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210814173632.png)

> Paper: https://arxiv.org/abs/2103.03231
>
> Github: https://github.com/facebookresearch/DONERF

自打伯克利和谷歌联合打造的**NeRF**横空出世，江湖上静态图变动图的魔法就风靡开来。

不过，想要像这样依靠AI来简化3D动态效果的制作，算力开销可不小：

以NeRF为例，想要在1440×1600像素、90Hz的VR头盔中实现实时渲染，需要37 petaFLOPS（每秒10^15次浮点运算）的算力——这在目前的GPU上根本不可能实现。

怎么降低点计算复杂度？

现在，来自奥地利格拉兹科技大学和Facebook的研究人员，就想出一招：引入真实深度信息。

就这一下，很快的，推理成本最高能降低**48倍**，并且只用1个GPU，就能以每秒20帧的速度实现交互式渲染。

画质什么的，也没啥影响，甚至还能有所提升

## 6. [Github1.3万星，迅猛发展的JAX对比TensorFlow、PyTorch](https://mp.weixin.qq.com/s/Vgs9VBos0jX2X2HNb_eYVw) | 机器之心

![image-20210814173925582](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210814173926.png)

> Github: https://github.com/google/jax

在机器学习领域，大家可能对 TensorFlow 和 PyTorch 已经耳熟能详，但除了这两个框架，一些新生力量也不容小觑，它就是谷歌推出的 JAX。很对研究者对其寄予厚望，希望它可以取代 TensorFlow 等众多机器学习框架。

JAX 最初由谷歌大脑团队的 Matt Johnson、Roy Frostig、Dougal Maclaurin 和 Chris Leary 等人发起。

JAX 是机器学习 (ML) 领域的新生力量，它有望使 ML 编程更加直观、结构化和简洁。

## 7. [技巧 | OpenCV程序执行时间计算](https://mp.weixin.qq.com/s/LieHmaGqS6HHC1nvGC1sUg) | 小白学视觉

大家用OpenCV做开发，经常需要调试算法，打印出算法的执行时间，OpenCV中没有直接获取时间戳的函数，但是有两个根据CPU时钟可以精准计算算法每个步骤执行时间的函数，通过它们可以计算一行或者多行代码的执行时间，视频处理的FPS等性能指标。

计算一段算法处理执行的时间秒数，代码结构如下：

```python
e1 = cv.getTickCount()
# your code execution
e2 = cv.getTickCount()
## 计算秒
time = (e2 - e1)/ cv.getTickFrequency()

## 计算毫秒
mt =((e2 - e1)/ cv.getTickFrequency())*1000
```

time是以秒位单位。

## 其他

- [Github标星13.6k ！一行代码从PDF提取Excel文件，牛逼炸了！](https://mp.weixin.qq.com/s/SZAZMknxh6VsDIHplr7dbA)

