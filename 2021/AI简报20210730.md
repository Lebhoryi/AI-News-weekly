[TOC]

> 导读： 本期为 AI 简报 20210730 期，请慢用~
>
> 本文一共 2800 字，通篇阅读结束需要 8~10 分钟

# AI 前沿

## 1. （轻量化）[超越MobileNet V3 | 详解SkipNet+Bias Loss=轻量化模型新的里程碑](https://mp.weixin.qq.com/s/BSCSGdQrQ1F1-5ofuH5lTA) | 集智书童

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730233911.webp)

> Paper: https://arxiv.org/abs/2107.11170
>
> Github: https://github.com/lusinlu/skipnet_evaluation

近年来，Compact卷积神经网络(CNNs)的性能有了显著的提高。然而，在参数众多的情况下，它们仍然无法提供与普通CNN相同的预测能力。这些层捕获的多样且丰富的特征是这些CNN的一个重要特征。

然而，大型CNN和小型CNN在这一特性上的差异很少被研究。在Compact CNN中，由于参数数量有限，不太可能获得丰富的特征，特征多样性成为本质特征。在模型推断期间，从数据点派生的激活映射中呈现的不同特征可能表明存在一组惟一描述符，这是区分不同类的对象所必需的。

相比之下，特征多样性较低的数据点可能无法提供足够数量的描述符来进行有效预测;作者称之为**随机预测**。随机预测会对优化过程产生负面影响，并损害最终性能。

本文提出通过重塑标准交叉熵来解决随机预测带来的问题，使其偏向具有有限数量独特描述特征的数据点。本文所提出的**新型Bias Loss**将训练重点放在一组有价值的数据点上，防止大量学习特征差的样本误导优化过程。

此外，为了说明多样性的重要性，作者提出了一系列**SkipNet模型**，其体系结构增加了最后一层的唯一描述符的数量。

实验表明，所提出的损失函数优于交叉熵损失。此外，与MobileNetV3 Large相比，Skipnet-M在相似的计算条件下，在ImageNet上分类准确率提高了1%。

## 2. （轻量化）[CSL-YOLO | 超越Tiny-YOLO V4，全新设计轻量化YOLO模型实现边缘实时检测！！！](https://mp.weixin.qq.com/s/KoaxIeycKCDsiIunOXC_VQ) |  集智书童

![image-20210730234111212](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730234112.png)

> Paper: https://arxiv.org/abs/2107.04829
>
> Github: https://github.com/D0352276/CSL-YOLO

由于计算资源有限，开发轻量级目标检测器是必要的。为了降低计算成本，如何生成冗余特征起着至关重要的作用。

本文提出了一种新的轻量级卷积方法——**Cross-Stage Lightweight(CSL)模块**，从简单的操作中生成冗余特征。在中间展开阶段用深度卷积代替逐点卷积来生成候选特征。所提出的CSL模块可以显著降低计算量。在MS-COCO上进行的实验表明，所提出的CSL-Module可以达到近似3x3卷积的拟合能力。

最后，利用该模块构建了轻量级检测器CSL-YOLO，在仅43% FLOPs和52%参数的情况下，实现了比TinyYOLOv4更好的检测性能。

## 3. [AS-MLP：上海科技&腾讯优图开源首个检测与分割领域MLP架构](https://mp.weixin.qq.com/s/ytZxqXyXMy2x7wM74mGQTg) | AIWalker

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730234154.webp)

> paper: https://arxiv.org/abs/2107.08391
>
> Code: https://github.com/svip-lab/AS-MLP

本文是上海科技大学在MLP架构方面的探索，它设计了一种轴向移位操作以便于进行空间信息交互。

在架构方面，AS-MLP采用了类似PVT的分层架构，因为可以轻易的迁移到下游任务。

所提方法在ImageNet数据集上取得了优于其他MLP架构的性能，在COC检测与ADE20K分割任务上取得了与Swin相当的性能。

值得一提的是，**AS-MLP是首个迁移到下游任务的MLP架构**。**注：CycleMLP与AS-MLP属于同一时期的工作，发到arxiv的时间也只差两天，说两者都是首个其实也可以。**

## 4. [无需检测器提取特征！LeCun团队提出MDETR：实现真正的端到端多模态推理｜ICCV 2021 Oral](https://mp.weixin.qq.com/s/6T0__6vEOAz72H8DRF6Evw) | 极市平台

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730235114.webp)

> 论文地址：https://arxiv.org/abs/2104.12763
>
> 代码地址：https://github.com/ashkamath/mdetr

目前，多模态推理模型大多都依赖于预先训练好的目标检测器来从图像中提取proposal。

然而检测器只能检测出固定类别的目标，这使得模型很难适应自由文本中视觉concept的长尾分布，因此本文提出了MDETR，一种端到端调制检测器，能够根据原始文本query直接来检测图像中的目标，

基于Transformer的结构，通过在模型的早期阶段融合这两种模态的信息，

来对文本和图像进行共同的推理。

最终，MDETR在检测和多个下游任务上都取得了SOTA的性能。

# 有趣的 AI 开源项目

## 1. [实践教程 | OpenVINO2021.4+YOLOX目标检测模型测试部署](https://mp.weixin.qq.com/s/v9Gs_Ab1dIELTIfTYYcB1A) | OpenCV学堂

之前有很多篇文章介绍 Yolox，没想到现在 Openvino 这么快支持了

- [吊打一切现有版本的YOLO！旷视重磅开源YOLOX：新一代目标检测性能速度担当！](https://mp.weixin.qq.com/s/mzMTMqwCpxDi54ULv5K2yg)
- [实践教程｜YOLOX目标检测ncnn实现](https://mp.weixin.qq.com/s/Sv8BXiXbvnEyzpVseY90VA)
- [【论文解读】YOLOX: Exceeding YOLO Series in 2021](https://mp.weixin.qq.com/s/PnfDnuEIPpSi_WJE2Xsvpw)

本文基于YOLOX的ONNX模型分别测试了YOLOX-Small与YOLOX-Tiny版本的模型。

**硬件配置与软件版本：**

```shell
Win10 64位
CPU CORE i7 8th
VS2017
OpenVINO2021.4
```

模型说明

两个模型的输入与输出格式分别如下：

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730233530.webp)

有三个输出层，分别是8倍、16倍、32倍的降采样，输出的8400计算方法为：

```shell
80x80+40x40+20x20 = 6400+1600+400=8400
```

分别对应640的8倍、16倍、32倍的降采样大小。

85的前四个是**cx、cy、w、h大小**，第五个是**object预测得分**，后面80个是COCO类别。

## 2. [实操教程｜使用 Opencv 简化面部地标检测](https://mp.weixin.qq.com/s/xN6Wf4VkqYo1BOWlPorCuA) | 极市平台

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730232942.webp)

> Github: https://github.com/BakingBrains/Face_LandMark_Detection

OpenCV 是用于计算机视觉、机器学习和图像处理的跨平台开源库，我们可以使用它来开发实时计算机视觉应用程序。它主要用于图像或视频处理以及分析，包括对象检测、面部检测等。

面部地标用于定位和表示面部的重要区域，例如：

- 嘴巴
- 眼睛
- 眉毛
- 鼻子
- 下颌线等

面部地标有许多应用，例如为人熟知的换脸、面部变形、头部姿势估计等。

今天我们将使用 OpenCV 和 MediaPipe 来检测图像中的468个面部地标。

## 3. [实时检测17个人体关键点，谷歌SOTA姿态检测模型](https://mp.weixin.qq.com/s/2Mk_FQoTB1wLmkyk5X865w) | 机器之心

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730232619.webp)

> 项目地址：https://github.com/tensorflow/tfjs-models/tree/master/pose-detection

不久之前谷歌研究院推出了最新的姿态检测模型 MoveNet，并在 TensorFlow.js 中推出了新的姿态检测 API，该模型可以非常快速、准确地检测人体的 17 个关键节点。这一开源项目对于姿态检测以及机器学习领域来说，是一件非常有意义的事。

最近，来自谷歌的研究者又更新了一个新的项目，该项目提供了包括 MoveNet 在内的多个可用于实时姿态检测的 SOTA 模型，目前该项目已有 3 种可选模型：

- MoveNet：是一种速度快、准确率高的姿态检测模型，可检测人体的 17 个关键点，能够以 50+ fps 的速度在笔记本电脑和手机上运行。
- BlazePose：MediaPipe BlazePose 可以检测人体 33 个关键点，除了 17 个 COCO 关键点之外，它还为脸部、手和脚提供了额外的关键点检测。
- PoseNet：可以检测多个姿态，每个姿态包含 17 个关键点。

## 4. [25行代码≈SOTA！OpenAI发布Triton编程语言，比PyTorch快2倍](https://mp.weixin.qq.com/s/XgJPHgDI5N-Cj_7urRQSBw) | 新智元

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730232754.webp)

> mmp, cuda 都还没有学会，就开始有了替代品

> Github: https://github.com/openai/triton

前段时间OpenAI才搞了个大新闻——AI编程神器Copilot。

这次，它又带来了能自动榨干GPU性能的编程语言——Triton。

速度要比PyTorch快两倍！

Triton到底有多强？

只要25行代码就能实现接近「SOTA」的性能！

内存合并，共享内存管理，SM内调度，Triton通通帮你搞定。

此外，Triton代码开源，兼容Python。

项目负责人Philippe Tillet表示：「我们的目标是让Triton成为深度学习中CUDA的替代品」。

不过，目前Triton 1.0仅支持Linux系统和英伟达的显卡。

AMD的显卡估计再等等也能用上，甚至也会支持CPU。

# 冠军方案

## 1. [CoTNet-重磅开源！京东AI Research提出新的主干网络CoTNet,在CVPR上获得开放域图像识别竞赛**冠军**](https://mp.weixin.qq.com/s/30lYjPZ9ZJAY9td6oZR0OQ) | 我爱计算机视觉
![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210730233809.png)

> 论文地址：https://arxiv.org/abs/2107.12292
> 代码地址：https://github.com/JDAI-CV/CoTNet
> 核心代码：https://github.com/xmu-xiaoma666/External-Attention-pytorch#22-CoTAttention-Usage

基于Self-Attention的Transformer结构，首先在NLP任务中被提出，最近在CV任务中展现出了非常好的效果。然而，大多数现有的Transformer直接在二维特征图上的进行Self-Attention，基于每个空间位置的query和key获得注意力矩阵，但相邻的key之间的上下文信息未得到充分利用。

本文设计了一种新的注意力结构CoT Block，这种结构充分利用了key的上下文信息，以指导动态注意力矩阵的学习，从而增强了视觉表示的能力。

作者将CoT Block代替了ResNet结构中的3x3卷积，来形成CoTNet，最终在一系列视觉任务（分类、检测、分割）上取得了非常好的性能，此外，CoTNet在CVPR上获得开放域图像识别竞赛冠军。

# 其他

- [详细解读SSPNet：小目标检测该如何进行改进？](https://mp.weixin.qq.com/s/AcqjO36bvT2YqWtCN6hEmg)

- [万字长文细说工业缺陷检测](https://mp.weixin.qq.com/s/m3953q_x3ekxuigVJ_oeyQ)

- [两个月，刷了八千篇Arxiv，我发现……](https://mp.weixin.qq.com/s/7wOtGOBPwBO6xgOyhefaNw)

