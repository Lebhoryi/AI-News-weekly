[TOC]

> 导读： 本期为 AI 简报 20210416 期，将为您带来 7 条相关新闻~
>
> 本文一共 2300 字，通篇阅读结束需要 5~8 分钟

# [1. 训练AI来玩CS反恐精英！清华朱军与剑桥博士后的这项研究太燃了 | 童年回忆](https://mp.weixin.qq.com/s/p_7c2IakrG8sCOLq5VjSQw) | AI科技评论

![Image](https://mmbiz.qpic.cn/mmbiz_gif/cNFA8C0uVPvicjQvHKsRXL4V6bWLXyqNhUxHUJtv1v6jicE7ZgpRfL7Cts37XFpOKruLnR4fZRH2jMRjn4ibgrRSQ/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

> 论文链接：https://arxiv.org/pdf/2104.04258.pdf

Fire in the hole !

在计算机内卷的时代当不了CS人才难道还当不了一名CS人才吗? 不知道你是不是这样想，我是这样想的

继 AI 在游戏界的一系列出色表现之后，如今 AI 终于对射击类游戏下手了 ！

近日，博士毕业于剑桥大学的Tim pearce与他的清华大学博士后导师朱军教授联合为《反恐精英：全球攻势》开发了一个AI 游戏智能体 —— CSGO 。

据了解，与其他游戏AI不同，CSGO没有使用API，而是采用了一种行为克隆(Behavioural Cloning)的新型解决方案，即从在线服务器上抓取大量人类游戏视频进行对战训练。

目前相关论文《基于大规模行为克隆的反恐精英死亡竞赛》已提交至arXiv上面。论文中指出，CSGO所使用的数据集约有400万帧，与ImageNet大小相当，这一规模比之前在FPS游戏中的模仿学习大了一个数量级。

开发一个 AI 智能体来进行CSGO完全竞争模式的挑战——研究人员认为这篇论文是迈向AI里程碑的一步。

遗憾的是，该项目暂未开源。

# [2. AI一键去纹身，几秒钟让你看见明星「真面目」](https://mp.weixin.qq.com/s/cf4Ysm-dzsAfgtTQP48qFA) | 机器之心

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210416172212.png)

> Github:
>
> https://github.com/vijishmadhavan/SkinDeep

有些时候，我们需要把一些人身上的纹身覆盖掉，以避免引人效仿。有的时候人们只是单纯地好奇，想知道一些大明星如果没有纹身会是什么样子。来自印度的机器学习研究者 Vijish Madhavan 最近开源的一个机器学习工具 SkinDeep 满足了我们的需求。

有人会问，为什么不把纹身直接 PS 掉？Photoshop 可以产生非常好的效果，但问题是使用 Photoshop 需要专业知识，如果用 PS 处理纹身的话，你可能需要花费几个小时的时间去修饰整个图像。

这一项目是由 Fast.AI 库构建的，你需要安装 fastai 1.0.61 版（及其依赖库），以及 PyTorch 1.6.0，不支持更高的版本。

尝试这一项目的最快方法就是在 Colab 上：

- https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep.ipynb

它的输出限制为 500 像素。

# [3. DIY一只“眼睛”摄像头看自己工作，能眨眼睛皱眉头，还能“撸”](https://mp.weixin.qq.com/s/m26QWBjOy_xa2p-lneULKQ) | 大数据文摘

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210416173233.png)

> 官网：
>
> https://marcteyssier.com/projects/eyecam/
>
> 开源链接：
>
> https://marcteys.github.io/eyecam/

最近，一位名叫Marc Teyssier的研究员就和他的团队在德国萨尔大学的人机交互实验室里，开发了这样一种不可思议的摄像头——Eyecam。一只肉嘟嘟、可以眨巴眼睛、还能皱眉头的“眼睛”。

不仅有眉毛、睫毛，连眼睛周围的细纹都清晰可见，个头儿也跟真实的眼睛差不多，整体大概是一个人的拳头大小。

除了看起来逼真，在放出的一段视频中，还看到它的“运动”状态-眨眼、扫视

甚至还可以情绪激动的“皱起眉头”。

**项目已开源，想做一只属于自己的眼睛吗？**

目前，这一项目的全部内容也已经在github上开源，主要使用到了以下几款软硬件的设计：

- 固件：Arduino程序，用于控制用于移动眼球，眼睑和眉毛的6个伺服电机。它经过优化，可以在Arduino Leonardo Pro Micro上运行。
- Raspberry Pi作为相机：如何将Raspberry Pi Zero + Pi Cam转换为标准USB相机
- Unity Control界面：附加界面，以视觉方式控制电机并运行计算机视觉处理

# [4. EfficientNetV2：更小，更快，更好的EfficientNet](https://mp.weixin.qq.com/s/NI4lznZG_t7XYfVkU_aw1g) | AI公园

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210416173602.png)

> **论文：https://arxiv.org/pdf/2104.00298.pdf**
>
> **代码：https://github.com/google/automl/efficientnetv2**

EfficientNets已经成为高质量和快速图像分类的重要手段。它们是两年前发布的，非常受欢迎，因为它们的规模让它们的训练速度比其他网络快得多。

几天前谷歌发布了EfficientNetV2，在训练速度和准确性方面都有了很大的提高。

通过渐进学习，我们的EfficientNetV2在ImageNet和CIFAR/Cars/Flowers数据集上显著优于之前的模型。

通过在相同的ImageNet21k上进行预训练，我们的EfficientNetV2在ImageNet ILSVRC2012上实现了87.3%的top1精度，在使用相同的计算资源进行5到11倍的训练时，比最近的ViT的准确率高出2.0%。

# [5. 霸榜多个CV任务，开源仅两天，微软分层ViT模型收获近2k star](https://mp.weixin.qq.com/s/PYMeo1ItnEP09ri7F_ITSg) | 机器之心

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210416173757.png)

> 论文链接：
>
> https://arxiv.org/pdf/2103.14030.pdf
>
> 项目地址：
>
> https://github.com/microsoft/Swin-Transformer

屠榜各大 CV 任务的微软 Swin Transformer，近日开源了代码和预训练模型。

不久前，微软亚研的研究者提出了一种通过移动窗口（shifted windows）计算的分层视觉 Transformer，他们称之为 Swin Transformer。相比之前的 ViT 模型，Swin Transformer 做出了以下两点改进：

其一，引入 CNN 中常用的层次化构建方式构建分层 Transformer；

其二，引入局部性（locality）思想，对无重合的窗口区域内进行自注意力计算。

Swin Transformer 论文公开没多久之后，微软官方于近日在 GitHub 上开源了代码和预训练模型，涵盖图像分类、目标检测以及语义分割任务。上线仅仅两天，该项目已收获 1900

# [6. Kaggle竞赛中使用YoloV5将物体检测的性能翻倍的心路历程 | 强烈推荐](https://mp.weixin.qq.com/s/LVICXBk8BMr1LbNbsvEO8A) | AI公园

> 英文原文：
>
> https://towardsdatascience.com/a-journey-of-building-an-advanced-object-detection-pipeline-doubling-yolov5s-performance-b3f1559463bf
>
> Github:
>
> https://github.com/mostafaibrahim17/VinBigData-Chest-Xrays-Object-detection-

作者在kaggle比赛中从建立基线到一步一步的优化过程，最终将performance提升了一倍，非常好的竞赛经验总结文章。

我花了三个月的时间深入研究物体检测。我尝试了很多方法，从实现最先进的模型，如YoloV5、VFNets、DETR，到将目标检测模型与图像分类模型融合以提高性能。在比赛的早期阶段，我努力提高基准模型的分数，但我找不到有用的在线资源，这就是我写这篇文章的原因。我想带你们踏上一段从头到尾的旅程，简要地向你们展示我所走的每一步，我的成绩几乎翻了一倍。

官方的竞赛指标是(mean) Average Precision，这是最常用的目标检测指标之一。为了向你展示每一步的进步，我将在旁边加上它的分数。

1. 第一步是建立一个简单的基线，0.126 mAP
2. 去掉一个输入类别！0.143 mAP (+13%)
3. 增加训练和推理图像的分辨率，0.169 mAP (+18%)
4. 融合EfficientNet和YoloV5，0.196 mAP (+16%)
5. 加权框融合(WBF)后处理，0.226 mAP (+15%)
6. 用5折交叉验证使用WBF融合，0.256 mAP (+13%)
7. 我尝试过的其他的东西，但是没有成功

# [7. 无需卷积，完全基于Transformer的首个视频理解架构TimeSformer出炉](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650810910&idx=3&sn=d9948c01f2a00ad99a5f3e7597000784&chksm=84e5ee60b3926776d7d916cbf8577d298285530b1749095504223008fd794aca87a109ad492f&scene=4&rd2werd=1#wechat_redirect) | 机器之心

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210416174033.png)

> 论文链接：
>
> https://arxiv.org/pdf/2102.05095.pdf

**TimeSformer 是首个完全基于 Transformer 的视频架构**。**无需卷积，训练速度快、计算成本低。**

近年来，Transformer 已成为自然语言处理（NLP）领域中许多应用的主导方法，包括机器翻译、通用语言理解等。

TimeSformer 在一些具有挑战性的动作识别基准（包括 Kinetics-400 动作识别数据集）上实现了最佳的性能。此外，与 3D 卷积神经网络（CNN）相比，TimeSformer 的训练速度大约快了 3 倍，而推断所需的计算量不足其十分之一。

此外，TimeSformer 的可扩展性使得在更长的视频片段上训练更大的模型成为可能。这为 AI 系统理解视频中更复杂的人类行为打开了大门，对需要理解人类复杂行为的 AI 应用来说是极为有益的。

无需卷积，训练速度快、计算成本低。

# 其他

[半导体行业DRAM深度研究报告：存储芯片研究框架 | 附完整报告下载](https://mp.weixin.qq.com/s/Uf6SPWe5hoMNpfpxQ4DK_w)

[开发板还能这么用？美国学者用Jetson Nano支持便携式AI假肢，控制每一根手指](https://mp.weixin.qq.com/s/ahaRhdrcf8emd_Xaw2ZjLA)

[马斯克脑机接口最新突破：猴子用意念玩游戏！](https://mp.weixin.qq.com/s/uRL5SDd72ou3CVQJQeIShQ)

[考驾照选择 AI 教练，心态稳定不会骂人](https://mp.weixin.qq.com/s/MySO4ObDipdauvL08gc2Ew)

[英伟达推出首个CPU！预计2023年正式投入使用](https://mp.weixin.qq.com/s/3Pcsq2bi2St4GBuozPAQkQ)