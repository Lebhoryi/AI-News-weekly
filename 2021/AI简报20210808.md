[TOC]

> 导读： 本期为 AI 简报 20210808 期，请慢用~
>
> AI 简报 Github 地址：https://github.com/Lebhoryi/AI-News-weekly
>
> 本文一共 2700 字，通篇阅读结束需要 8~10 分钟

# AI 前沿

## 1.【轻量化】[YOffleNet | YOLO V4 基于嵌入式设备的轻量化改进设计](https://mp.weixin.qq.com/s/NnCk7b9cTkD5DLZl-o3oyQ) |  集智书童

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808162437.webp)

> 论文：https://arxiv.org/abs/2108.00392

最新的基于CNN的目标检测模型相当精确，但需要高性能GPU实时运行。对于内存空间有限的嵌入式系统来说，它们在内存大小和速度方面依旧不是很好。

由于目标检测是在嵌入式处理器上进行的，因此在保证检测精度的同时，最好尽可能地压缩检测网络。有几个流行的轻量级检测模型，但它们的准确性太低。因此，本文提出了一种新的目标检测模型 **YOffleNet**，该模型在压缩率高的同时，将精度损失降到最小，可用于自动驾驶系统上的实时安全驾驶应用。该模型的Backbone架构是基于YOLOv4实现，但是可以用ShuffleNet的轻量级模块代替CSP的高计算负荷的DenseNet，从而大大压缩网络。

在KITTI数据集上的实验表明，提出的YOffleNet比YOLOv4-s压缩了4.7倍，在嵌入式GPU系统(NVIDIA Jetson AGX Xavier)上可以达到46FPS的速度。与高压缩比相比，精度略有降低，为85.8% mAP，仅比YOLOv4-s低2.6%。因此，提出的网络具有很高的潜力部署在嵌入式系统。

## 2. 【屠榜】[屠榜目标跟踪！大连理工和MSRA提出STARK：基于Transformer的目标跟踪器 | ICCV 2021](https://mp.weixin.qq.com/s/Tlrhoj2jnexu9mjRky0yww) | CVer

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808161940.webp)

**Learning Spatio-Temporal Transformer for Visual Tracking**

作者单位 大连理工大学，微软亚洲研究院

论文：https://arxiv.org/abs/2103.17154

代码：**https://github.com/researchmm/Stark**

**短时跟踪**

- **TrackingNet AUC 82.0% (目前第一)!**
- **GOT-10K AO 68.8% (只用GOT10K训练 目前第一)！**
- **VOT2020 EAO 0.505 (目前第二)！**

**长时跟踪**

- **LaSOT AUC 67.1% (目前第一)!**
- **VOT2020-LT F-score 70.2% (目前第一)!**
- **OxUvA MaxGM 78.2% (Leaderboard第二，有paper的工作中第一)！**

## 3. [ICCV 2021 | 全面超越ResNet！NUS和依图开源T2T-ViT：Transformer又一力作](https://mp.weixin.qq.com/s/IemBXxsPcoGgEYmRF3GMLw) | 极市平台

> 代码链接：https://github.com/yitu-opensource/T2T-ViT
>
> 论文：https://arxiv.org/abs/2101.11986

Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808161801.webp)



> 代码链接：https://github.com/yitu-opensource/T2T-ViT
>
> 论文：https://arxiv.org/abs/2101.11986

本文是依图科技在ViT方面的一次突破性的探索。与之前ViT、Detr、Deit等不同之处在于：本文针对ViT的特征多样性、结构化设计等进行了更深入的思考，提出了一种新颖的Tokens-to-Token机制，用于同时建模图像的局部结构信息与全局相关性，同时还借鉴了CNN架构设计思想引导ViT的骨干设计。最终，**仅仅依赖于ImageNet数据，而无需JFT-300M预训练，所提方案即可取得全面超越ResNet的性能**，且参数量与计算量显著降低；与此同时，在轻量化方面，所提方法**只需简单减少深度与隐含层维度即可取得优于精心设计的MobileNet系列方案的性能**。强烈推荐各位同学研究一下该文。

# 有趣的开源项目

## 4. [用一张草图创建GAN模型，新手也能玩转，朱俊彦团队新研究入选ICCV 2021](https://mp.weixin.qq.com/s/pXsMJEhHRdYFYOqd4oYiCQ) | 机器之心

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808161404.webp)



- 论文地址：https://arxiv.org/pdf/2108.02774.pdf
- 项目地址：https://peterwang512.github.io/GANSketching

朱俊彦等来自 CMU 和 MIT 的研究者提出 GAN Sketching，该方法通过一个或多个草图重写 GAN，让新手用户更容易地训练 GAN。具体地，该方法还能通过用户草图改变原始 GAN 模型的权重，并且通过跨域（cross-domain ）对抗损失鼓励模型输出来匹配用户草图。

此外，该研究还探索了不同的正则化方法，以保持原始模型的多样性和图像质量。

实验表明，GAN Sketching 可以塑造 GAN 来匹配草图指定的形状和姿态，同时保持真实感和多样性。研究者最后演示了生成的 GAN 的一些应用，包括潜在空间插值和图像编辑等应用。

## 5. [惊呆了！一键秒变迪丽热巴！ DeepFaceLive让直播换脸真假难辨](https://mp.weixin.qq.com/s/C7MPrV8mOGNp6zZ2Ul0EIA) | CVer

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808162106.webp)

DeepFaceLive项目地址：https://github.com/iperov/DeepFaceLive

AI换脸技术「船新」升级，DeepFaceLive推出实时换脸软件，一键安装，新手友好，换脸后丝毫看不出破绽。

欢迎光临「DeepFace整容所」！

无需开刀！无需微创！

双眼皮、开眼角、瘦脸统统都不在话下！

让你秒变帅哥美女！

现在网络上95%以上的Deep Fake视频都是用DeepFaceLab制作的。

DeepFaceLive详细教程可以访问：https://github.com/iperov/DeepFaceLive/blob/master/doc/setup_tutorial_windows/index.md

DeepFaceLab论文地址：https://arxiv.org/pdf/2005.05535.pdf

DeepFaceLab项目地址：https://github.com/iperov/DeepFaceLab

## 6. [这个GAN没见过猪，却能把狗变成猪](https://mp.weixin.qq.com/s/8prslNSRh2UuQLXUvv8avQ) | 量子位

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808162326.png)

论文地址：
https://arxiv.org/abs/2108.00946
GitHub地址：
https://github.com/rinongal/StyleGAN-nada

**不用成千上万张目标图片训练**，就能让GAN生成你想要的图片，有可能吗？

还真有可能！

来自特拉维夫大学和英伟达的研究人员成功地**盲训**出领域自适应的图像生成模型——**StyleGAN-NADA**。

也就是只需用简单地一个或几个字描述，一张目标领域的图像也不需要，StyleGAN-NADA就能在几分钟内训练出你想要的图片：

比如现在在几张狗狗的基础图片上输入“Sketch”，**不到1分钟**，一张张草图风格狗的图片就出来了。(视频没有声音可放心“食用”)

## 7. [亲手制作一个《哈利·波特》人物图谱，原来罗恩和赫敏的姻缘从第一部就已注定？](https://mp.weixin.qq.com/s/cIj2Z6-EaN_EOywq3FiGVw) | 大数据文摘

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808162541.webp)

《哈利·波特》人物图谱GitHub链接：

https://github.com/tomasonjo/blogs/blob/master/harry_potter/HarryPotterNLP.ipynb

文摘菌记得小时候看《哈利·波特》小说的时候，最难记住的就是那些音译的名字，又长又多，最后只能关注那几个主要人物，跟着主要剧情一路过去，当个爽文看完了。

这就导致一些边缘人物根本没关注到，也错过了J·K·罗琳埋下的许多小伏笔。

比如卢娜与韦斯莱其实是邻居关系，这个在《火焰杯》中众人出发去世界杯时有伏笔，但是很少有人第一遍看的时候能注意到。

大概是为了从一开始就厘清人物关系，Medium上一位博主Tomaz Bratanic开发了一个小项目，用Selenium结合SpaCy来创建一个Neo4j哈利·波特人物图谱，**把《哈利·波特》第一部中所有的人物都纳入一张网络中，**人物关系一目了然。

# 福利

## 8. [台大李宏毅《机器学习》2021课程撒花完结！除了视频、PPT，还有人汇编了一本答疑书](https://mp.weixin.qq.com/s/V713VGW3924OikPNu_qxJA) | 机器之心

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808161507.webp)

- 课程主页：https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html
- 机器之心知识站入口：https://app6ca5octe2206.pc.xiaoe-tech.com/detail/p_6049e1c6e4b05a6195befd56/6

今年 2 月末，「精灵宝可梦大师」李宏毅的《机器学习》最新一期课程正式[开课](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650810240&idx=1&sn=ff9491ffcb91cf35e8006e13603fae43&scene=21#wechat_redirect)。对于想要入门机器学习的同学来说，这是一门不容错过的经典课程，视频、PPT 等资料都可以在课程页面找到，而且授课语言是中文。

在内容方面，这门课程重点讲解的是深度学习。虽然深度学习是一门相对进阶的技术，但李宏毅老师表示，这不会改变这门课「机器学习入门课」的属性，仍然会让绝大多数人听得懂，「你可以将它作为你机器学习的第一门课」。如果你还学过林轩田的《机器学习基石与技法》，你会发现这两门课其实可以很好地衔接。

学习手册：

- 书籍地址：https://rentruewang.github.io/learning-machine/intro.html
- GitHub 地址：https://github.com/rentruewang/learning-machine

## 9. [盘点一下后训练量化的基本操作](https://mp.weixin.qq.com/s/Ib9WAGbdCMmi0WOHuuP9ww) | AI小男孩

这篇文章就简单聊聊后训练量化的一些常规操作。

一些基础知识

在此之前，还是需要先了解一下后训练量化 (下面简称 PTQ，Post-training Quantization) 是啥？具体细节这里就不展开了，不熟悉的读者欢迎看回我之前的文章 ([神经网络量化入门--后训练量化](https://mp.weixin.qq.com/s?__biz=Mzg4ODA3MDkyMA==&mid=2247483717&idx=1&sn=c98980a322db18ff77842029090a3943&chksm=cf81f722f8f67e342e401e7250c9545fcdf617e160feec32b9b886553a9b5377cc9c1b232a85&token=1589349128&lang=zh_CN&scene=21#wechat_redirect))。简单来说，后训练量化就是在不重新训练网络 (即不更新 weight) 的前提下，获取网络的量化参数。

说到量化参数，就不得不祭出量化的基本公式了 (假设用 8bit 非对称量化)：

![image-20210808162726861](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808162728.png)

## 10. [Anchor-free目标检测综述 -- Keypoint-based篇 (2020)](https://mp.weixin.qq.com/s/fgtMp6vIepnvKbudujlZmA) |  晓飞的算法工程笔记

早期目标检测研究以anchor-based为主，设定初始anchor，预测anchor的修正值，分为two-stage目标检测与one-stage目标检测，分别以Faster R-CNN和SSD作为代表。后来，有研究者觉得初始anchor的设定对准确率的影响很大，而且很难找到完美的预设anchor，于是开始不断得研究anchor-free目标检测算法，意在去掉预设anchor的环节，让网络自行学习anchor的位置与形状，在速度和准确率上面都有很不错的表现。anchor-free目标检测算法分为两种，一种是DenseBox为代表的Dense Prediction类型，密集地预测的框的相对位置，另一种则是以CornerNet为代表的Keypoint-bsaed Detection类型，以检测目标关键点为主。
  本文主要列举几种Keypoint-based Detection类型的网络，主要涉及以下网络：

![image-20210808162813576](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210808162814.png)

# 其他

- [AI 发展月报（2021年7月）](https://mp.weixin.qq.com/s/QDEaA_RbU1G_ExW9XsrHdA) | 学术头条

- [直击行业痛点！端侧模型部署的成熟解决方案有了！](https://mp.weixin.qq.com/s/CGChNKiJEbX5HQqwfaK4ng) | 夕小瑶的卖萌屋

- [卖萌屋招人啦](https://mp.weixin.qq.com/s/LJ-xBB6EY_IXalLVEzpNNw) |  夕小瑶的卖萌屋
- [CV十年发展之观察：1.5万篇论文透视「业界」与「学界」，到底谁更胜一筹？](https://mp.weixin.qq.com/s/x8oxtYlb-P_rttNhjsLi9Q) | 机器之心
- [四年磨一剑，谷歌自研Tensor芯片即将登陆Pixel旗舰手机，秋季开卖](https://mp.weixin.qq.com/s/8t4NVgJBG3Kf1qpOpFvL9A) | 机器之心

